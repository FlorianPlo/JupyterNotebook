{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3393057-ea06-4f2d-94c7-9697fccd3a41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9397067b9282bf77aa677b093849d83d",
     "grade": false,
     "grade_id": "cell-8f51681d0010e7ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***DISCLAIMER (Read this carefully)***\n",
    "\n",
    "Before you turn this assignment in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel\n",
    "Restart) and then run all cells (in the menubar, select Cell\n",
    "Run All). Do NOT add any cells to the notebook!\n",
    "\n",
    "Do not forget to submit both the notebook AND the files in the data/ subfolder according to the CoC!\n",
    "Make sure you fill in any place that says YOUR CODE HERE or YOUR ANSWER HERE , as well as your name and group below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4ced-9b69-4c8e-a7d0-12265cc28eea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccbfb02fdc67dc903abb412ab000bf72",
     "grade": false,
     "grade_id": "cell-01518f30645ea1ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2 (Group)\n",
    "When carrying out a Data-Science project, screening and selecting appropriate data sources for the tasks at hand comes at the beginning. This assignment is about accessing and characterising potential data sources in teams of three. The teams have been randomly assigned. BEWARE! In Assignment 5, you will be asked to provide answers to those questions. Make sure that combining the two datasets makes sense from an analytical perspective!\n",
    "\n",
    "-----\n",
    "## Step 0 (2 points)\n",
    "\n",
    "Find two data sets online (from one or several sources) that would be interesting to combine and create ***data citations*** as Python dictionaries. \n",
    "\n",
    "The data sets should fulfill the following requirements:\n",
    "\n",
    "* Each data set must have a different file format (either CSV, XML, or JSON), please choose\n",
    "  - one CSV file (dataset1) \n",
    "  - and one JSON or XML file (dataset2)\n",
    "\n",
    "* The two datasets should not be two variations of each other (i.e. simply the same dataset for two different regions or timeframes or from the same source just in two different formats)\n",
    "* Workable data-set sizes: The selected or extracted data sets should have thousands of entries (>= 1000), but not more than (<=) 10000 entries. Be \"entries we mean rows or distinguishable key-value pairs). If larger, use an excerpt from the original data set. Justify in detail the extraction criteria in the markdown cell below and \n",
    "  1) add the code used for the extraction in the code cell or describe how you filtered the sample  \n",
    "  2) make the extracted dataset also available at a downloadable URL (for instance in a Github repository, [here](https://raw.githubusercontent.com/AxelPolleres/simple_dataset_sharing_repo/main/test.csv)'s an example)\n",
    "  3) name the new `resourceURL` in the data citation.\n",
    "* You may start from (but you are not limited to) the resource collections hinted at [in the Unit 2 slides](https://datascience.ai.wu.ac.at/ws21/dataprocessing1/unit2.html#slide-53).\n",
    "\n",
    "* Important: The use of datasets from kaggle.com and other curated collections of datasets with accompanying tutorials on processing and analysis (as highlighted to you in Unit 2) is **discouraged**. You are required to use **primary data sources**: This is mainly because we want you to work on data sets that have not been processed with some analysis in mind, so that you show that you can handle (messy) data sets harvested on the brownfields of Data Science. Besides, such curated datasets have been repeatedly used in ready-made case and tutorial work, which makes it basically impossible for us to establish whether your submissions are genuine contributions of yours. There is one viable option: Work backwards from the Kaggle data set to the original data source, obtain updated data from there, and start from there.\n",
    "\n",
    "\n",
    "* Please adhere to the CoC.\n",
    "\n",
    "[Data citations](http://blogs.nature.com/scientificdata/2016/07/14/data-citations-at-scientific-data/) must contain the following details:\n",
    "- creator: provider organisation / author(s) of the data set, e.g. \"Zentralanstalt für Meteorologie und Geodynamik (ZAMG)\"\n",
    "- catalogName: Names of the data repository and/or the Open Data portal used, e.g. Open Data Österreich\"\n",
    "- catalogURL: URL of th repository / portal, e.g. \"https://www.data.gv.at/\"\n",
    "- datasetID: (specific to the data repository), e.g. \"https://www.data.gv.at/katalog/dataset/zamg_meteorologischemessdatenderzamg\"\n",
    "- resourceURL: a URL where the CSV, XML or JSON file can be downloaded, e.g. \"https://www.football-data.co.uk/new/JPN.csv\"\n",
    "- pubYear: Dataset publication year, i.e. since when it is published, e.g. \"2012\"\n",
    "- lastAccessed: when have you last accessed the dataset (i.e. datetime of accessing, obtaining a copy of the data set) in ISO Format? e.g. \"2021-03-08T13:55:00\"\n",
    "\n",
    "One final note: as mentioned above, if you want to use a repository for your file download (e.g. github), you are allowed to do that. The most important part is that the URL can be accessed stably for each dataset you have chosen. \n",
    "\n",
    "Store the data citation in a dictionary for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ac2b05-1102-4cf7-96a3-f1ec47459ece",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b6ee8b15f725340d21f67892d57f1cf",
     "grade": true,
     "grade_id": "cell-cea2669d70d8d76a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset1= {\n",
    "    \"creator\" : \"U.S. Department of Health & Human Services\" ,\n",
    "    \"catalogName\" : \"US Data Catalog\" ,\n",
    "    \"catalogURL\" : \"catalog.data.gov.us\" ,\n",
    "    \"datasetID\" : \"5320bb9d-7476-4408-a410-e6ca30f13b3c\" ,\n",
    "    \"resourceURL\" : \"https://raw.githubusercontent.com/Zedtisfying/DP1-Set-1/main/Indicators_of_Anxiety_and_Depression.json\"  ,\n",
    "    \"pubYear\" : \"2020\"  ,\n",
    "    \"lastAccessed\" : \"2024-04-16T12:29:36\",\n",
    "}\n",
    "\n",
    "dataset2= {\n",
    "    \"creator\" : \"U.S. Department of Health & Human Services\" ,\n",
    "    \"catalogName\" : \"US Data Catalog\" ,\n",
    "    \"catalogURL\" : \"catalog.data.gov.us\" ,\n",
    "    \"datasetID\" : \"732f92fd-4775-44c9-a228-3cd2121abe82\" ,\n",
    "    \"resourceURL\" : \"https://raw.githubusercontent.com/Zedtisfying/DP1-Set-1/main/Indicators_of_Health_Insurance_Coverage_at_the_Time_of_Interview_(Last).csv\"  ,\n",
    "    \"pubYear\" : \"2020\"  ,\n",
    "    \"lastAccessed\" : \"2024-04-16T12:30:58\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852966b3-7ce6-47e0-a879-2de74b756861",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39009fd5ac6f4d6b559120b873b92451",
     "grade": true,
     "grade_id": "cell-d3cc293b6207d1c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "import traceback\n",
    "import sys\n",
    "import os\n",
    "\n",
    "assert_equal(type(dataset1), dict)\n",
    "assert_equal(type(dataset2), dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c1bfa-decc-4711-83c1-87c761bda55e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bd1475fa3535b9a58bf34f0a20219c7",
     "grade": false,
     "grade_id": "cell-a34ab569805a650e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following structure for your answer below:\n",
    "\n",
    "Data set 1\n",
    "\n",
    "(Describe the source and the general content of the dataset and why you chose it)\n",
    "\n",
    "Data set 2\n",
    "\n",
    "(Describe the source and the general content of the dataset and why you chose it)\n",
    "\n",
    "Project ideas\n",
    "\n",
    "(Describe in your own words, which kind of tasks could be addressed by combining the selected data sets, esp. how the two data sets fit together and what complementary information they contain; Formulate a question that could be potentially answered by combining data from both datasets; how could the data sets be combined exactly? 250 words max. BEWARE! In Assignment 5, you will be asked to provide answers to those questions. Make sure that combining the two datasets makes sense from an analytical perspective!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04cd31-a41c-4d42-b756-87567ed08ee8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25e589c21292c28ecbe9193e15daa7b9",
     "grade": true,
     "grade_id": "cell-9bc09f21e0c42050",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Data set 1\n",
    "\n",
    "During the initial phases of the pandemic, the U.S. Census Bureau, in partnership with five federal agencies, launched the Household Pulse Survey to gather data on the social and economic effects of Covid-19. This JSON file provides insights into the mental impacts, specifically depression and anxiety, categorized by state and documented over various timeframes.The provided file has been shortened to fit the task requirements, and had the statistical values removed to keep the most relevant parts.\n",
    "\n",
    "Data set 2\n",
    "\n",
    "The second dataset consists of the results of a survey, in which the applicants were asked their insurance state. This CSV file consists of the answers to this survey documented over different timeframes. The file has been shortened to cover only specific time frame to match with the first dataset additionally the columns including the calculations of statistical values and intervalls have been removed.\n",
    "\n",
    "Project ideas: This project aims to investigate the influence of insurance coverage on the severity of anxiety symptoms among residents across different states during the Covid-19 pandemic. By merging two datasets—one detailing anxiety severity by state and timeframe and the other illustrating insurance status categorized by numbers within the same timeframe—this study seeks to uncover potential correlations between mental health outcomes and access to healthcare resources at a regional level. These two datasets will be combined based on the state variables to examine whether certain states exhibit higher levels of anxiety symptoms and whether residents' insurance coverage varies by state. The inclusion of multiple timeframes allows for comparisons of anxiety levels within the same states over different periods. With the provided datasets we aim to answer how the insurance coverage impacts the severity of anxiety symptoms among residents of different states during the Covid-19 pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a43ce55-9c0b-44b2-a833-8e4e107175da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8978a8e226a4b0c92d461641d71447",
     "grade": false,
     "grade_id": "cell-426859b26b9c84e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## Step 1 - File Access (3 points)\n",
    "\n",
    "Write a Python function `accessData` that takes the dataset dictionary created in step 0 as an input and returns an extended dictionary including following additions:\n",
    "\n",
    "* Write code that accesses the dataset from its `resourceURL` using the python `requests` package:\n",
    " * detects whether it's and XML, CSV or JSON file by\n",
    "     * checking whether the download URL **ends** with suffix \"xml\", \"json\", \"csv\" \n",
    "     * checking whether the \"Content-Type\" HTTP header field contains information about the format, hinting on XML, JSON or CSV, i.e., check whether the substring XML, JSON or CSV appears in the \"Content-Type\" header in either upper- or lowercase. \n",
    " * Detects the file size from the HTTP header (converted to KB) of each data set, clearly documenting your actions (e.g. through commented code).\n",
    "\n",
    "The result of the code below should extend your dictionaries `dataset1` and `dataset2` with two keys named \n",
    "* `\"detectedFormat\"` (which has one of the following values: `\"XML\"`, `\"JSON\"`, `\"CSV\"`, or `\"unknown\"`, if nothing could be detected from checking the suffix or HTTP header, or if the information in both was inconsistent)\n",
    "* and `\"filesizeKB\"` which contains the filesize in KB (Conversion should be done accordingly to decimal SI prefixes) from the number of bytes in the header-information. If there is no respective header information return 0.\n",
    "* If the detected format is `\"unknown\"`, the expected filesize to be returned is also 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06da511-204d-4316-bc92-507835acbe27",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ce47a1d2b5782eb291c725baea5db29",
     "grade": false,
     "grade_id": "cell-87173edcb1445261",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def accessData(datadict):\n",
    "    # YOUR CODE HERE\n",
    "    URL = datadict[\"resourceURL\"]\n",
    "    \n",
    "# check if URL ENDS with suffix \n",
    "    # split URL by dots so that suffix is at index -1 \n",
    "    suffix = URL.split(\".\")\n",
    "    \n",
    "    \n",
    "    # what do they mean by \"ends with suffix\"? only .json or also .json?accessType=DOWNLOAD\n",
    "    for ftype in [\"csv\", \"json\", \"xml\"]:\n",
    "        if ftype in suffix[-1]:\n",
    "            detectedFormatURL = ftype\n",
    "            break\n",
    "        else: detectedFormatURL = \"unknown\"\n",
    "\n",
    "# checks header information\n",
    "    r = requests.head(URL)\n",
    "    rhead = r.headers\n",
    "    \n",
    "    # \"detectedFormat\" (which has one of the following values: \"XML\", \"JSON\", \"CSV\", or \"unknown\", if nothing could be detected from checking the suffix or HTTP header, or if the information in both was inconsistent)\n",
    "    # \"if the information is both was inconsistent\"\n",
    "    # is it also inconsistent when we dont get a value for content-type? (key content-type not available)\n",
    "    conttype = rhead[\"content-type\"].lower() if \"content-type\" in rhead else \"unknown\"  #detectedFormatURL <- davor # else unknown <- Jonas\n",
    "    contleng = int(rhead[\"content-length\"])/1000 if \"content-length\" in rhead else 0\n",
    "    \n",
    "   \n",
    "    detectedFormatHeader = \"\"\n",
    "    # checks if file name strings are substrings of the \"content-type\" header value \n",
    "    for ftype in [\"csv\", \"json\", \"xml\"]:\n",
    "        if ftype in conttype:\n",
    "            detectedFormatHeader = ftype\n",
    "            break\n",
    "        else: detectedFormatHeader = \"unknown\"\n",
    "    \n",
    "    valid_formats = [\"csv\", \"json\", \"xml\"]\n",
    "    \n",
    "    # checks if the two file formats found in the suffix and the header information are the same and creates new dictionary pairs\n",
    "    ## 4 possibilities\n",
    "        ## if both are the same and in valid_formats return that\n",
    "        ## One is in valid_formats and one is unknown return the one in validFormats\n",
    "        ## both unknown --> unknown\n",
    "        ## both are in valid_formats but not the same --> unknown\n",
    "    if detectedFormatURL in valid_formats and detectedFormatURL == detectedFormatHeader:\n",
    "        datadict.update({\"detectedFormat\": detectedFormatURL.upper(), \"filesizeKB\": contleng})\n",
    "    \n",
    "    elif detectedFormatURL == \"unknown\" and detectedFormatHeader == \"unknown\":\n",
    "        datadict.update({\"detectedFormat\": \"unknown\", \"filesizeKB\": 0})\n",
    "        \n",
    "    elif detectedFormatURL in valid_formats and detectedFormatHeader in valid_formats and detectedFormatURL != detectedFormatHeader:\n",
    "        datadict.update({\"detectedFormat\": \"unknown\", \"filesizeKB\": 0})\n",
    "    \n",
    "    elif detectedFormatURL in valid_formats:\n",
    "        datadict.update({\"detectedFormat\": detectedFormatURL.upper(), \"filesizeKB\": contleng})\n",
    "    \n",
    "    elif detectedFormatHeader in valid_formats:\n",
    "        datadict.update({\"detectedFormat\": detectedFormatHeader.upper(), \"filesizeKB\": contleng})\n",
    "   \n",
    "    \n",
    "   \n",
    "    return datadict\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba078d8-b700-4cb8-8a18-4a2fa86dc210",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d95fb7a29ffb9cea68ca365e17633265",
     "grade": true,
     "grade_id": "cell-09b529ecf9606ac6",
     "locked": true,
     "points": 0.24,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Basic tests to see if your solution meets the foundational demands described in the task description\n",
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "dataset1= accessData(dataset1)\n",
    "dataset2= accessData(dataset2)\n",
    "assert_in(dataset1[\"detectedFormat\"], [\"XML\", \"JSON\", \"CSV\", \"unknown\"])\n",
    "assert_in(dataset2[\"detectedFormat\"], [\"XML\", \"JSON\", \"CSV\", \"unknown\"])\n",
    "assert_true(isinstance(dataset1[\"filesizeKB\"], (int, float)))\n",
    "assert_true(isinstance(dataset2[\"filesizeKB\"], (int, float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c70a0c-34d4-47db-952b-46d4cb3bf347",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580596224539603a63b4a559ba37d01f",
     "grade": true,
     "grade_id": "cell-b1ef1760abd62f3f",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fa6983-d7c6-431a-89a9-c86ac7a0451d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c795cbd00b10bbb32830e3026ed4f676",
     "grade": true,
     "grade_id": "cell-b16c0da0f2b8dc22",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bd86f0-1a8d-4270-9b79-ad4ad967266b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfc68663db636e99afeb58486c87a1aa",
     "grade": true,
     "grade_id": "cell-e19b418cde2ba027",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d6fbc1-2eea-4017-acbc-4cd735cccab5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e6d3529936b739da626bdd9098061ca",
     "grade": true,
     "grade_id": "cell-278bfc1d2c26772d",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64df8a63-b6c3-4d4a-83a8-e4e79f9cd122",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3537723c82c9905d481fbe710834686a",
     "grade": true,
     "grade_id": "cell-1fb00904a60db5c7",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac351364-b28e-4cde-9423-8aa4f9b437bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5c98a2910050eb9f43a5d55f44ef1dc",
     "grade": true,
     "grade_id": "cell-affc2fa2a6b001c9",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859d99e8-bbb2-4729-937d-0c7ada7e4047",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16708dc91e800626b08b2e3703170884",
     "grade": true,
     "grade_id": "cell-00e4aadd3923d75d",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6b1413-ccd8-4c90-aa55-cc0910f880e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d776ae85271b5fedf7a8a24b2154506",
     "grade": true,
     "grade_id": "cell-d0566ccfef366cdd",
     "locked": true,
     "points": 0.22,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa69593-670f-4b2c-b5d0-1c70c9970b92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98e00d798908db7a765a41c1c61b7548",
     "grade": false,
     "grade_id": "cell-dcd3aa38b0b6d216",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please explain your findings, using the following structure for your answer below (in \"other remarks\" you can explain, for instance, why you think your code did not detect the correct format, if needed)\n",
    "\n",
    "Data set 1\n",
    "\n",
    "(format, size, other remarks)\n",
    "\n",
    "Data set 2\n",
    "\n",
    "(format, size, other remarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ffd91-c334-43fd-aa68-ae9aeb9ae24b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56e8a3407f0b2570544c4791d8468d5f",
     "grade": true,
     "grade_id": "cell-40248ad63aa2baea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Data set 1 \n",
    "\n",
    "CSV file, 65.646kB, created by U.S. Department of Health & Human Services, from US Data Catalog(catalog.data.gov.us), datasetID 732f92fd-4775-44c9-a228-3cd2121abe82, URL https://raw.githubusercontent.com/Zedtisfying/DP1-Set-1/main/asdfasdfasdfasdfasdf.csv, published in 2020 last accessed on 2024-04-16T12:29:36.\n",
    "\n",
    "Data set 2\n",
    "\n",
    "JSON file, 12.499kB,created by U.S. Department of Health & Human Services, from US Data Catalog(catalog.data.gov.us), datasetID 5320bb9d-7476-4408-a410-e6ca30f13b3c URL https://raw.githubusercontent.com/Zedtisfying/DP1-Set-1/main/Indicators_of_Anxiety_and_Depression.json, published in 2020 last accessed on 2024-04-16T12:30:58."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26a4ed-ce75-4a64-8c94-0eff3cb8de7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8ba2987efce440ec7e6f4d4857bd88e",
     "grade": false,
     "grade_id": "cell-c236c8ec72c4dded",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 2  (5 points) - Format Validation\n",
    "\n",
    "Establish that the two data files obtained are well-formed according to the detected data format (CSV, JSON, or XML). That is, the syntax used is valid according to accepted syntax definitions. Are there any violations of well-formedness?\n",
    "\n",
    "\n",
    "Proceed as follows (for each data file, in turn): according to the \"suspected\" data format from Step 1:\n",
    "\n",
    "  1. Use an _online validator_ for CSV, XML, and JSON, respectively, to confirm whether the files you downloaded in Step 1 are well-formed for the respective file format, document your findings and modify the file as described: \n",
    "\n",
    "   a. **Case 1**: no well-formedness errors were detected: \n",
    "    * Generally describe at least 3 well-formedness checks that your data sets, depending on its \"suspected\" format (against the background knowledge of Unit 2) should fulfill;\n",
    "    * Store a local copy of the file called `data_notebook-[notebook-nr.]_[name].[file extension]` in the `data/` subfolder\n",
    "    * Create another local copy of your data file called `data_notebook-[notebook-nr.]_[name]-invalid.[file extension]` and introduce a selected well-formedness violation (one occurrence) therein;\n",
    "    * document that the online validator you used finds the error you introduced\n",
    "\n",
    "   b. **Case 2**: well-formedness errors occurred:\n",
    "    * Document the occurrences by printing out the error message and describe the types of well-formedness violation that were reported to you.\n",
    "    * Store a local copy called `data_notebook-[notebook-nr.]_[name]-invalid.[file extension]`  in the `data/ subfolder`\n",
    "    * Create another local copy called `data_notebook-[notebook-nr.]_[name].[file extension]`, of your data file that fixes the well-formedness violations therein manually.  \n",
    "    \n",
    "**Please note that the datasets in the `data/` subfolder are for documentation only. Do not access those for subsequent steps!**\n",
    "    \n",
    "\n",
    "  2. Write a Python function `parseFile(datadict, format)` that that accesses the dataset from its `resourceURL`. The dataset should then be checked accordingly the given parser for the parameter `format` to check the following:\n",
    "     * CSV: Returns `True`, if a consistent delimiter out of `\",\",\";\",\"\\t\"` can be detected, such that each row has the same (> 1) number of elements, otherwise False\n",
    "     * JSON: Returns `True` if the file can be parsed with the `json` package, catching any parsing exceptions.\n",
    "     * XML: Returns `True` if the file can be parsed with the `xmltodict` package, catching any parsing exceptions.\n",
    "     * Returns `False` if any other format is supplied by the parameter.\n",
    "     \n",
    "In order to handle parsing exceptions and errors from the used packages, you can use [catching exceptions](https://docs.python.org/3/tutorial/errors.html), such that the program does not simply fail to check whether the file is parseable as the format specified in `format`    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d77f70-6543-4aab-9306-010a706fd09b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5a2e097de799a7b80df50520dad2457",
     "grade": false,
     "grade_id": "cell-57dc295d9edd354d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following structure for your answer in the cell below to document **Step 2.1**:\n",
    "\n",
    "***Data set 1***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n",
    "\n",
    "***Data set 2***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec87d67-75e8-4bc7-9502-f24598959a5e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b03b52aa01bb4c3dd985217938fc69c4",
     "grade": true,
     "grade_id": "cell-7326ff597819ce15",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Data set 1\n",
    "\n",
    "online validator,https://jsonchecker.com/,\n",
    "\n",
    "error message:\n",
    "\n",
    "\"Parse error on line 1: Indicator,Group,Stat ^ Expecting 'STRING', 'NUMBER', 'NULL', 'TRUE', 'FALSE', '{', '[', got 'undefined'\"\n",
    "\n",
    "The description for the fifth column was missing. It has been replaced with the corresponding column indicator. Additionally, one row containing invalid data types for certain columns, along with empty cells in other columns, has been removed from the dataset for data integrity purposes.\n",
    "\n",
    "Data set 2\n",
    "\n",
    "online validator,https://onlinetools.com/csv/validate-csv ,\n",
    "\n",
    "error message:\n",
    "\n",
    "\"Error 1 title: CSV Contains Empty Lines message: The file contains empty lines. row: 2654\"\n",
    "\n",
    "Missing value due to formatting issues while editing the file, found the distinct values on the file and replaced the empty cell with the only missing state for that time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5bfb28-0a4f-4fbe-a2ed-4d8e9c3d52d8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eed04eccfe7b9897bc09aa0b00b66d27",
     "grade": false,
     "grade_id": "cell-e72d44bc996d8aaf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import xmltodict\n",
    "\n",
    "def parseFile(datadict, format):\n",
    "\n",
    "    # create important variables \n",
    "    dataformat = datadict[\"detectedFormat\"]\n",
    "    response = requests.get(datadict[\"resourceURL\"])\n",
    "    \n",
    "    # VALITATE CSV CONTENT\n",
    "    if format == dataformat == \"CSV\":\n",
    "        \n",
    "        # list of delimiter to be checked\n",
    "        delimiter = [\",\",\";\",\"\\t\"]\n",
    "        # gets the text of the URL and dumps each line into a list\n",
    "        text = response.text.splitlines()\n",
    "        best_delimiter = None\n",
    "        \n",
    "        for d in delimiter:\n",
    "            # tries if any of the values of delimiter are eligable for a delimiter\n",
    "            try:\n",
    "                # reads the text of the file with a possible delimiter of the delimiter list \n",
    "                reader = csv.reader(text, delimiter = d)\n",
    "    \n",
    "                for row in reader:\n",
    "                    # create a set with all the row lengths. used characteristic that the set has no duplicates (len(set) = 1 if all rows are same length)\n",
    "                    row_len = {len(row)}\n",
    "\n",
    "                # find best dilimiter\n",
    "                # 1. condition checks if all row lengths are the same\n",
    "                # 2. condition checks if there are multiple colums in the csv file (transform set into list to access through indices)\n",
    "                if len(row_len) == 1 and list(row_len)[0] > 1: \n",
    "                    best_delimiter = d                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "        if best_delimiter:\n",
    "            csv_reader = csv.reader(text, delimiter = best_delimiter)\n",
    "                \n",
    "            return True\n",
    "        \n",
    "    \n",
    "    # VALITATE JSON CONTENT\n",
    "    elif format == dataformat == \"JSON\":\n",
    "        # try if content of the dataset could be parsed as a json file\n",
    "        try: \n",
    "            data_json = response.json()\n",
    "           \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            \n",
    "            return False\n",
    "        \n",
    "    # VALITATE XML CONTENT\n",
    "    elif format == dataformat == \"XML\":\n",
    "        # try if content of the dataset could be parsed as a xml file        \n",
    "        try:\n",
    "            data_xml = xmltodict.parse(response.content)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            \n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce5b307-c87b-4906-afc6-9c1febd645e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8dba04d64b7a6dd6cedc9434b6ec78d7",
     "grade": true,
     "grade_id": "cell-44a0730c406d4f1f",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "assert_equal([parseFile(dataset1, \"XML\"),\n",
    "    parseFile(dataset1, \"JSON\"),\n",
    "    parseFile(dataset1, \"CSV\"),\n",
    "    parseFile(dataset2, \"XML\"),\n",
    "    parseFile(dataset2, \"JSON\"),\n",
    "    parseFile(dataset2, \"CSV\")].count(True), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e2c763-6dcd-4ab4-9cf1-07d376bbf933",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b1707e0eafd11b04790a361420d22f8",
     "grade": true,
     "grade_id": "cell-a6fe57057ce3e18e",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7089c06-e16f-4372-8a41-7ac26ed76f45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d7ea4929d3fa5b005a9913921d9a2a8",
     "grade": true,
     "grade_id": "cell-aa7fd9b40f36f086",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f1b12f1-52e3-4ffc-9cb8-a8ed3f635891",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82e57ee635b1b3b871c3ef9f1f3cc9b1",
     "grade": true,
     "grade_id": "cell-7d845903b5b4589e",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca9bedd-01f2-41c5-aad8-04e5701fae0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb76deee21dbae2ae9299edf7443ba9",
     "grade": true,
     "grade_id": "cell-d647097f451b66c2",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27211989-c7bb-42e8-8d3b-cd5286758d7a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8022febe031b0a602b2a33443b001bbc",
     "grade": true,
     "grade_id": "cell-f77b7d77dfebf6fc",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c822dd-fa67-436d-9f2c-3652682ffbd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8007b3d48186c30912d246c3bc6023a3",
     "grade": true,
     "grade_id": "cell-f7ec87ba2b1d4805",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "069798e6-e2f8-4c44-96d4-7cbf1f189275",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd605035aa02f06cfd9bb967aca1fa38",
     "grade": true,
     "grade_id": "cell-0872aa870b41a0f9",
     "locked": true,
     "points": 0.375,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14967cf4-e8da-4e9a-90a9-752f1f441915",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "121f1b6432df1a273435a4617379de23",
     "grade": false,
     "grade_id": "cell-c0772a5952f0b1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 3 - Content analysis (5 points)\n",
    "\n",
    "Similar to the Python function `parseFile(datadict,format)` above, now create a new Python function `describeFile(datadict)` that analyses the given file according to the respective format detected in Step 1 and returns a dictionary containing the following information:\n",
    "\n",
    "* for CSV files: number of columns, number of rows, column number (from 0 to n) of the column which contains the longest text. You do not have to try to transform any string to integer or float, simply take the values as is from the csv file. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfColumns:\"  ...,\n",
    "       \"numberOfRows\":  ... ,\n",
    "       \"longestColumn\" : ... }\n",
    "    ```\n",
    "\n",
    "* for JSON files: number of different attribute names, nesting depth, length of the longest list appearing in an attribute value. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"longestListLength\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `longestListLength` should be set to 0 if no list appears. [Nesting depth](https://www.tutorialspoint.com/find-depth-of-a-dictionary-in-python) is defined as follows: \n",
    "   * a flat JSON object with only atomic attribute values has depth 1. \n",
    "   * a JSON attribute with another object as value (or another oject as member of a list value!) increases the depth by 1\n",
    "   * and so on.\n",
    "\n",
    "\n",
    "* for XML files: number of different element and attribute a names (i.e. the sum of both), nesting depth, maximum numeric value in the dataset. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfElementsAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"maxNumericValue\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `maxNumericValue` should be set to 0 if there are no numberic values present. Nesting depth is defined as the nesting depth of elements.\n",
    "  \n",
    "For files that cannot be parsed with respective given format, the function should simply return an empty dictionary (`{}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4494b82-73ef-410f-8646-ce960e85fae7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e05f4df55ec10447c9c1c3dbc5e19dc",
     "grade": false,
     "grade_id": "cell-223d8521820ec726",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def describeCSV(datadict):\n",
    "    URL = datadict[\"resourceURL\"]\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "    # Decode content to text\n",
    "        csv_content = response.content.decode('utf-8')\n",
    "        csv_reader = csv.reader(csv_content.splitlines(), delimiter=',')\n",
    "        \n",
    "        num_rows = 0\n",
    "        num_columns = 0\n",
    "        rows = []  # dont print this - too big, crashed my jupyter \n",
    "    \n",
    "        for row in csv_reader:\n",
    "            rows.append(row)\n",
    "            num_rows += 1\n",
    "            \n",
    "        if rows:\n",
    "            num_columns = len(rows[0])\n",
    "        \n",
    "        max_len = \"\"\n",
    "        col_of_max_len = 0\n",
    "\n",
    "        for x in rows:\n",
    "            for col_index, value in enumerate(x):\n",
    "                if len(value) > len(max_len):\n",
    "                    max_len = value\n",
    "                    col_of_max_len = col_index\n",
    "                   \n",
    "             \n",
    "        return {\n",
    "            \"numberOfColumns:\": num_columns,\n",
    "            \"numberOfRows:\":num_rows,\n",
    "            \"longestColumn:\": col_of_max_len,\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(\"Failed CSV Download:\", response.status_code)\n",
    "\n",
    "def finddepthJSON(datadict): ### https://www.tutorialspoint.com/find-depth-of-a-dictionary-in-python, 18.04.2024\n",
    "    if isinstance(datadict, dict):\n",
    "        return 1 + (max(map(finddepthJSON, datadict.values()))\n",
    "            if datadict else 0)\n",
    "    return 0\n",
    "\n",
    "def JsonLongestList(datadict):\n",
    "    longestList = 0\n",
    "    \n",
    "    for x in datadict:\n",
    "        if isinstance(datadict, list):\n",
    "            if len(x) > longestList:\n",
    "                longestList = len(x)\n",
    "    return longestList\n",
    "\n",
    "def describeJSON(datadict):\n",
    "    URL = datadict[\"resourceURL\"]\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        json_data = response.json()\n",
    "        \n",
    "        \n",
    "        CountAttributes = len(json_data[0]) ## Counting the attributes in the datafile\n",
    "       \n",
    "        return {\n",
    "            \"numberOfAttributes:\": CountAttributes,\n",
    "            \"nestingDepth:\": finddepthJSON(json_data[0]),\n",
    "            \"longestListLength\": JsonLongestList(json_data[0])\n",
    "        }\n",
    "     \n",
    "    else:\n",
    "            print(\"Failed JSON Download:\", response.status_code)\n",
    "\n",
    "def finddepthXML(datadict): ### https://www.tutorialspoint.com/find-depth-of-a-dictionary-in-python, 18.04.2024\n",
    "    if isinstance(datadict, dict):\n",
    "        return 1 + (max(map(finddepthXML, datadict.values()))\n",
    "            if datadict else 0)\n",
    "    return 0\n",
    "\n",
    "def maxValue(a):\n",
    "    max_value = 0\n",
    "    \n",
    "    def recursion(obj):\n",
    "        # access non local variables to modify them within the new function\n",
    "        nonlocal max_value\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                recursion(value)\n",
    "        if isinstance(obj, list):\n",
    "            for element in obj:\n",
    "                recursion(element)\n",
    "        value = 0\n",
    "        try:\n",
    "            value = float(obj)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "        # adds every integer to the counter\n",
    "        if isinstance(value, (int, float)) and value > max_value:\n",
    "            max_value = value\n",
    "\n",
    "    recursion(a)\n",
    "    result = (max_value)\n",
    "    return(result)\n",
    "\n",
    "def Count_Attributes_Elements(datadict):\n",
    "    elements_attributes_Counter = 0\n",
    "    if isinstance(datadict, dict):\n",
    "        elements_attributes_Counter += len(datadict)\n",
    "        for key, value in datadict.items():\n",
    "            if isinstance(value, dict):                   \n",
    "                elements_attributes_Counter += Count_Attributes_Elements(value)\n",
    "            elif isinstance(value, list):\n",
    "                for i in value:\n",
    "                    elements_attributes_Counter += Count_Attributes_Elements(i)\n",
    "                    Count_Attributes_Elements(value)\n",
    "        return elements_attributes_Counter            \n",
    "\n",
    "def describeXML(datadict):\n",
    "    URL = datadict[\"resourceURL\"]\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        xml_data = response.text\n",
    "        xml_dict = xmltodict.parse(xml_data)\n",
    "        \n",
    "        return {\n",
    "            \"numberOfAttributes:\":Count_Attributes_Elements(xml_dict),\n",
    "            \"nestingDepth:\": finddepthXML(xml_dict),\n",
    "            \"maxNumericValue\": maxValue(xml_dict)\n",
    "        }\n",
    "       \n",
    "    else:\n",
    "        print(\"Failed XML Download\")\n",
    "        return none            \n",
    "\n",
    "def describeFile(datadict):\n",
    "    if datadict[\"detectedFormat\"] == \"CSV\":\n",
    "        return describeCSV(datadict)\n",
    "    elif datadict[\"detectedFormat\"] == \"JSON\":\n",
    "        return describeJSON(datadict)\n",
    "    elif datadict[\"detectedFormat\"] == \"XML\":\n",
    "        return describeXML(datadict)\n",
    "    else:\n",
    "        return {}\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e58d31-e57e-4ac6-84d3-48809e6b0d04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fc953a755d35d8c3255d3772e083002",
     "grade": true,
     "grade_id": "cell-eba7f348525a45dd",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "assert_equal(len(describeFile(dataset1)), 3)\n",
    "assert_equal(len(describeFile(dataset2)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27d737cb-7aa3-4748-93f0-13f0c08d342b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2637ef320c218287599c8a8bc6cb63b",
     "grade": true,
     "grade_id": "cell-557eee89020c4510",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb7ad07b-dc5c-4434-b06d-075591fe46f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69a118b251422690870222cf5f042bb9",
     "grade": true,
     "grade_id": "cell-b125d2724a7003c1",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1dbd6cf-d518-4b65-83b1-f36203b9e13c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0633205721d85edf11023800bffeaac2",
     "grade": true,
     "grade_id": "cell-c366624cc0d29728",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e49feaf-64c0-4cd8-a9a8-98b29daf66e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58bb482889a5b09576fee3a30e6a295",
     "grade": true,
     "grade_id": "cell-2644ceec88376e7e",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "017ede3e-7e5e-4f38-b9ca-c7254e5aa881",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2c8ba0c7a08c002c55f1fa22670ce27",
     "grade": true,
     "grade_id": "cell-75825a6fa21b6764",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c25532-33c7-4ea6-bbcb-1f23e0df07ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "309f12a43f4b8dcfeb3d324bbb15386e",
     "grade": true,
     "grade_id": "cell-1c787c4413548bab",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dff5f404-fac2-46de-a8f1-444c906bb443",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "154b2599ee47b9c72a65428699e22484",
     "grade": true,
     "grade_id": "cell-2e575d64dfdbaa9b",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70e38881-63b9-4f02-b301-30f7b96476d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c740615f5f15f8aa8493a689ec475f",
     "grade": true,
     "grade_id": "cell-4218dd06c8ea8833",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "649287cc-d9d3-422b-8a47-25295ad8440a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9ecd21614153a677cbea1387012cb86",
     "grade": true,
     "grade_id": "cell-91e3b4cbf0931744",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44dac52-f1fe-4f5f-8d99-b27462a52af2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98cefcea6e84a1f4020f6d6c8edb0fd9",
     "grade": true,
     "grade_id": "cell-6fb522b91576b08f",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dec0094-1b71-4931-ba84-a3f8a6ee8230",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ffb610f2bde1f7c2eaceb6efb8268fe",
     "grade": true,
     "grade_id": "cell-5bab3319a1f792f9",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7db940ce-f01a-4546-9e64-ce97ff0880fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4e54339e82a940891d1ecd304105500",
     "grade": true,
     "grade_id": "cell-2213630186aef937",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fd6b71a-fd95-44f9-ad81-4ede19519cde",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc27fdeb6d52906b0f90f2e16d106c07",
     "grade": true,
     "grade_id": "cell-136ca0d44e2fd03d",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c433c150-9c7c-4fd6-a162-4112d9d219b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c03b7b23ac2048f0f560fa2d28f5d97",
     "grade": true,
     "grade_id": "cell-024d562b2d63af48",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c4e48b9-fc7f-4e36-b450-f64c767513f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2ed892a49dd9f7d2a0aa4d2d1fce759",
     "grade": true,
     "grade_id": "cell-469f1889794c4746",
     "locked": true,
     "points": 0.1875,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT DELETE OR CHANGE THIS CELL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e1e37-e968-436f-ac91-a22154e6f32b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c2f16e71ca35453c63daed0b1106c6e",
     "grade": false,
     "grade_id": "cell-66aada55df321ea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***Final check:***\n",
    "\n",
    "Be sure to cross-check the results by\n",
    "1) manually inspecting your chosen dataset and\n",
    "2) comparing the results for plausibility against the results of your code... \n",
    "\n",
    "Describe your findings and use the following structure for your answer below:\n",
    "\n",
    "*Data set 1*\n",
    "\n",
    "(number and types of items etc., describe your findings)\n",
    "\n",
    "*Data set 2*\n",
    "\n",
    "(number and types of items etc., describe your findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bce1c-e495-4733-baa8-ddc2962f077f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df18cdf1882599d295da8465485a5b1c",
     "grade": true,
     "grade_id": "cell-4709b3248c430b6a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Data set 1\n",
    "\n",
    "{'numberOfAttributes:': 7, 'nestingDepth:': 1, 'longestListLength': 0}\n",
    "These results are correct for our Files and seem to be correct for a few other Testfiles\n",
    "\n",
    "Data set 2\n",
    "\n",
    "{'numberOfColumns:': 10, 'numberOfRows:': 2653, 'longestColumn:': 0}\n",
    "These results are correct for our Files and seem to be correct for a few other Testfiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
